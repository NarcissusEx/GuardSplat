{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from functools import reduce, partial\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "from utils.graphics_utils import focal2fov, getProjectionMatrix\n",
    "from scene.cameras import MiniCam\n",
    "from utils.loss_utils import ssim\n",
    "from utils.image_utils import psnr\n",
    "from lpipsPyTorch import lpips\n",
    "from gaussian_renderer import render\n",
    "from scene import Scene, GaussianModel\n",
    "\n",
    "item = 'lego'\n",
    "msg_len = 32\n",
    "source_dir = 'eval_examples'\n",
    "save_map = False # you can visualize the views if set this flag as \"True\"\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        # evaluation args\n",
    "        self.msg_len = 32\n",
    "        self.source_dir = source_dir\n",
    "        self.item = item\n",
    "        self.msg_len = msg_len\n",
    "        self.batch_size = 32\n",
    "        # basic 3DGS args\n",
    "        self.sh_degree = 3\n",
    "        self.white_background = True\n",
    "        self.data_device = \"cuda\"\n",
    "        self.convert_SHs_python = False\n",
    "        self.compute_cov3D_python = False\n",
    "        self.debug = False\n",
    "        self.save_map = save_map\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "def normalize(output):\n",
    "    output[output > 0.5] = 1\n",
    "    output[output <= 0.5] = 0\n",
    "    return output.bool()\n",
    "\n",
    "def accuarcy(output, target):\n",
    "    output = normalize(output)\n",
    "    err = torch.logical_xor(output, target).sum() / target.numel()\n",
    "    return (1 - err) * 100.\n",
    "\n",
    "def getSE3(r, t):\n",
    "    SE3 = np.eye(4)\n",
    "    SE3[:3, :3] = np.array(r)\n",
    "    SE3[:3, 3 ] = np.array(t)\n",
    "    return SE3\n",
    "\n",
    "def getTestCameras(source_dir, item):\n",
    "    with open(os.path.join(source_dir, f'cameras-{item}.json')) as jf:\n",
    "        clist = json.load(jf)\n",
    "\n",
    "    w2cs = [np.linalg.inv(getSE3(c['rotation'], c['position'])).transpose() for c in clist]\n",
    "    w2cs = torch.from_numpy(np.array(w2cs)).cuda().float()\n",
    "    params = {\n",
    "        'width'  : clist[0]['width'],\n",
    "        'height' : clist[0]['height'],\n",
    "        'fovx'   : focal2fov(clist[0]['fx'], clist[0]['width']),\n",
    "        'fovy'   : focal2fov(clist[0]['fy'], clist[0]['height']),\n",
    "        'znear'  : 0.01,\n",
    "        'zfar'   : 100.0\n",
    "    }\n",
    "\n",
    "    proj_matrix = getProjectionMatrix(znear=params['znear'], zfar=params['zfar'], fovX=params['fovx'], fovY=params['fovy']).transpose(0,1).cuda()\n",
    "    projs = w2cs.bmm(proj_matrix.repeat(len(w2cs), 1, 1))\n",
    "    return [MiniCam(world_view_transform=w2c, full_proj_transform=proj, **params) for w2c, proj in zip(w2cs, projs)]\n",
    "\n",
    "def extract_rendered_views_and_gts(gaussians, guardsplat, cameras, renderArgs):\n",
    "    with torch.no_grad():\n",
    "        pds, gts = [], []\n",
    "        for viewpoint in cameras:\n",
    "            pds.append(torch.clamp(render(viewpoint, guardsplat, *renderArgs)[\"render\"], 0.0, 1.0)[None])\n",
    "            gts.append(torch.clamp(render(viewpoint, gaussians, *renderArgs)[\"render\"], 0.0, 1.0)[None])\n",
    "            \n",
    "        return torch.cat(pds), torch.cat(gts)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_image_similarity(pds, gts, args):\n",
    "    PSNR, SSIM, LPIPS = [], [], 0\n",
    "    with torch.no_grad():\n",
    "        for idx in range((len(gts) + args.batch_size - 1) // args.batch_size):\n",
    "            PSNR.append(psnr(pds[idx * args.batch_size : (idx + 1) * args.batch_size], gts[idx * args.batch_size : (idx + 1) * args.batch_size]).cpu())\n",
    "            SSIM.append(ssim(pds[idx * args.batch_size : (idx + 1) * args.batch_size], gts[idx * args.batch_size : (idx + 1) * args.batch_size], size_average=False).cpu())\n",
    "            LPIPS += lpips(pds[idx * args.batch_size : (idx + 1) * args.batch_size], gts[idx * args.batch_size : (idx + 1) * args.batch_size]).item()\n",
    "        \n",
    "    return {\n",
    "        'PSNR'  : torch.cat(PSNR).mean().item(),\n",
    "        'SSIM'  : torch.cat(SSIM).mean().item(),\n",
    "        'LPIPS' : LPIPS / len(pds)\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_bit_accuracy(pds, message, model, args):\n",
    "    decoded_messages = []\n",
    "    with torch.no_grad():\n",
    "        for idx in range((len(pds) + args.batch_size - 1) // args.batch_size):\n",
    "            decoded_messages.append(model(pds[idx * args.batch_size : (idx + 1) * args.batch_size].half()))\n",
    "    return accuarcy(torch.cat(decoded_messages), message.repeat(len(pds), 1)).item()\n",
    "\n",
    "def getSize(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def save_maps(imglist, method_names, source_dir):\n",
    "    from PIL import Image\n",
    "    import cv2\n",
    "    save_dir = os.path.join(args.source_dir, 'results')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    N, H, W, C = imglist[0].shape\n",
    "    (_, th), _ = cv2.getTextSize('f', cv2.FONT_HERSHEY_DUPLEX, 2, 2)\n",
    "    tws = [cv2.getTextSize(method_name, cv2.FONT_HERSHEY_DUPLEX, 2, 2)[0][0] for method_name in method_names]\n",
    "    \n",
    "    for idx, imgs in enumerate(zip(*imglist)):\n",
    "        L, T = 0, 0\n",
    "        merge = np.full((H + 2 * th, W * 2, C), 255, dtype=np.uint8)\n",
    "        for img, method_name, tw in zip(imgs, method_names, tws):\n",
    "            merge[T : T + H, L : L + W] = img\n",
    "            merge = cv2.putText(merge, method_name, (L + W // 2 - tw // 2, H + int(1.5 * th)), cv2.FONT_HERSHEY_DUPLEX, 2, (0, 0, 0), 2)\n",
    "            L += W\n",
    "        Image.fromarray(np.uint8(merge)).save(os.path.join(save_dir, f'{str(idx).zfill(4)}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Everything\n",
    "\n",
    "# Loading 3DGS models\n",
    "gaussians = GaussianModel(args.sh_degree)\n",
    "gaussians.load_ply(os.path.join(args.source_dir, f'original-{args.item}.ply'))\n",
    "\n",
    "# Loading Watermarked 3DGS models\n",
    "guardsplat = GaussianModel(args.sh_degree)\n",
    "guardsplat.load_ply(os.path.join(args.source_dir, f'watermarked-{args.item}.ply'))\n",
    "\n",
    "bg_color = [1, 1, 1] if args.white_background else [0, 0, 0]\n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "# Loading message decoder\n",
    "message_decoder = torch.jit.load(os.path.join(args.source_dir, f'CLIP_visual+msg_decoder-{args.msg_len}.pt')).eval().cuda()\n",
    "\n",
    "# Loading Message\n",
    "with open(os.path.join(args.source_dir, f'message-{args.item}.txt'), 'r') as txtfile:\n",
    "    message_text = txtfile.read()\n",
    "    message = torch.tensor([float(x) for x in message_text]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Parameter Size of Different Modules:\n",
      "CLIP Visual Encoder : 83.78M\n",
      "Our Msg Decoder     : 0.20M\n",
      "\n",
      "Message : 11001000001100000110111001010111\n",
      "\n",
      "Evaluating 200 views on Lego scene w.r.t N_L=32 bits\n",
      "\n",
      "Bit Accuracy between Original and Watermarked Models:\n",
      "[GuardSplat (Ours)] Bit Acc : 98.453125\n",
      "[Original 3DGS]     Bit Acc : 65.40625\n",
      "\n",
      "Image Similarity against Original Model:\n",
      "PSNR : 41.3957 | SSIM : 0.9962 | LPIPS : 0.0013\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "print (f'The Parameter Size of Different Modules:')\n",
    "print (f'CLIP Visual Encoder : {getSize(message_decoder.CLIP_visual) / 1024 / 1024:.2f}M')\n",
    "print (f'Our Msg Decoder     : {getSize(message_decoder.msg_decoder) / 1024 / 1024:.2f}M')\n",
    "\n",
    "print (f'\\nMessage : {message_text}')\n",
    "cameras = getTestCameras(args.source_dir, args.item)\n",
    "pds, gts = extract_rendered_views_and_gts(gaussians, guardsplat, cameras, (args, background))\n",
    "\n",
    "print (f'\\nEvaluating {len(cameras)} views on {args.item.capitalize()} scene w.r.t N_L={args.msg_len} bits')\n",
    "\n",
    "# bit accuracy between the original 3DGS and our GuardSplat\n",
    "acc_ours = eval_bit_accuracy(pds, message, message_decoder, args)\n",
    "acc_3dgs = eval_bit_accuracy(gts, message, message_decoder, args)\n",
    "print (f'\\nBit Accuracy between Original and Watermarked Models:')\n",
    "print (f'[GuardSplat (Ours)] Bit Acc : {acc_ours}')\n",
    "print (f'[Original 3DGS]     Bit Acc : {acc_3dgs}')\n",
    "\n",
    "# visual similarity on the original model-rendered views\n",
    "ans = eval_image_similarity(pds, gts, args)\n",
    "atext = reduce(lambda x1, x2 : f'{x1} | {x2}', [f'{k} : {v:.4f}' for k, v in ans.items()])\n",
    "print (f'\\nImage Similarity against Original Model:')\n",
    "print (atext)\n",
    "\n",
    "if args.save_map:\n",
    "    save_maps([np.uint8(x.permute(0, 2, 3, 1).cpu().numpy() * 255.) for x in [gts, pds]], ['Original 3DGS', 'GuardSplat (Ours)'], args.source_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
